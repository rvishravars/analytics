name,owner,url,stars,forks,description,topics,language,created_at,updated_at,has_training,training_files_sample,has_testing,testing_files_sample,open_issues,license
minGPT,karpathy,https://github.com/karpathy/minGPT,23117,3029,A minimal PyTorch re-implementation of the OpenAI GPT (Generative Pretrained Transformer) training,[],Python,2020-08-17T07:08:48Z,2025-12-10T03:49:11Z,True,['mingpt/trainer.py'],True,['tests/test_huggingface_import.py'],79,MIT License
sentence-transformers,huggingface,https://github.com/huggingface/sentence-transformers,17979,2715,State-of-the-Art Text Embeddings,[],Python,2019-07-24T10:53:51Z,2025-12-10T04:42:17Z,True,"['docs/cross_encoder/pretrained_models.md', 'docs/cross_encoder/training/examples.rst', 'docs/cross_encoder/training_overview.md', 'docs/cross_encoder/training_overview.md', 'docs/img/adaptive_pre-training.png']",True,"['.github/workflows/tests.yml', 'docs/img/backends_benchmark_cpu.png', 'docs/img/backends_benchmark_gpu.png', 'docs/img/ce_backends_benchmark_cpu.png', 'docs/img/ce_backends_benchmark_gpu.png']",1339,Apache License 2.0
trl,huggingface,https://github.com/huggingface/trl,16587,2342,Train transformer language models with reinforcement learning.,[],Python,2020-03-27T10:54:55Z,2025-12-10T03:17:17Z,True,"['.github/issue_template/new-trainer-addition.yml', 'docs/source/bco_trainer.md', 'docs/source/cpo_trainer.md', 'docs/source/distributing_training.md', 'docs/source/dpo_trainer.md']",True,"['.github/workflows/slow-tests.yml', '.github/workflows/tests-experimental.yml', '.github/workflows/tests.yml', '.github/workflows/tests_latest.yml', 'tests/__init__.py']",602,Apache License 2.0
Swin-Transformer,microsoft,https://github.com/microsoft/Swin-Transformer,15517,2202,"This is an official implementation for ""Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"".","['ade20k', 'image-classification', 'imagenet', 'mask-rcnn', 'mscoco', 'object-detection', 'semantic-segmentation', 'swin-transformer']",Python,2021-03-25T12:42:36Z,2025-12-10T02:28:18Z,True,"['configs/simmim/simmim_finetune__swin_base__img224_window7__800ep.yaml', 'configs/simmim/simmim_finetune__swinv2_base__img224_window14__800ep.yaml', 'configs/simmim/simmim_pretrain__swin_base__img192_window6__800ep.yaml', 'configs/simmim/simmim_pretrain__swin_base__img192_window6__800ep.yaml', 'configs/simmim/simmim_pretrain__swinv2_base__img192_window12__800ep.yaml']",True,['kernels/window_process/unit_test.py'],202,MIT License
detr,facebookresearch,https://github.com/facebookresearch/detr,14945,2639,End-to-End Object Detection with Transformers,[],Python,2020-05-26T23:54:52Z,2025-12-10T04:47:58Z,True,"['d2/train_net.py', 'd2/train_net.py']",True,"['datasets/coco_eval.py', 'datasets/panoptic_eval.py', 'test_all.py']",255,Apache License 2.0
Megatron-LM,NVIDIA,https://github.com/NVIDIA/Megatron-LM,14492,3358,Ongoing research training transformer models at scale,"['large-language-models', 'model-para', 'transformers']",Python,2019-03-21T16:15:52Z,2025-12-10T05:16:53Z,True,"['examples/academic_paper_scripts/detxoify_lm/finetune_gpt.py', 'examples/academic_paper_scripts/detxoify_lm/finetune_gpt_distributed-1.3b.sh', 'examples/bert/train_bert_340m_distributed.sh', 'examples/bert/train_bert_340m_distributed.sh', 'examples/gpt3/train_gpt3_175b_distributed.sh']",True,"['.github/workflows/_build_test_publish_wheel.yml', '.github/workflows/build-test-publish-wheel.yml', '.github/workflows/cicd-approve-test-queue.yml', '.github/workflows/install-test.yml', '.github/workflows/trigger-mbridge-tests.yml']",572,Other
RWKV-LM,BlinkDL,https://github.com/BlinkDL/RWKV-LM,14214,978,"RWKV (pronounced RwaKuv) is an RNN with great LLM performance, which can also be directly trained like a GPT transformer (parallelizable). We are at RWKV-7 ""Goose"". So it's combining the best of RNN and transformer - great performance, linear time, constant space (no kv-cache), fast training, infinite ctx_len, and free sentence embedding.","['attention-mechanism', 'chatgpt', 'deep-learning', 'gpt', 'gpt-2', 'gpt-3', 'language-model', 'linear-attention', 'lstm', 'pytorch', 'rnn', 'rwkv', 'transformer', 'transformers']",Python,2021-08-08T06:05:27Z,2025-12-10T04:01:03Z,True,"['rwkv-v1/src/trainer.py', 'rwkv-v1/train.py', 'rwkv-v1/train.py', 'rwkv-v2-rnn/src/trainer.py', 'rwkv-v2-rnn/train.py']",True,"['rwkv-v7/misc/lambada_test.jsonl', 'rwkv-v7/mmlu_test_dataset/data-00000-of-00001.arrow', 'rwkv-v7/mmlu_test_dataset/dataset_info.json', 'rwkv-v7/mmlu_test_dataset/state.json', 'rwkv-v7/rwkv_mmlu_eval.py']",141,Apache License 2.0
segmentation_models.pytorch,qubvel-org,https://github.com/qubvel-org/segmentation_models.pytorch,11145,1810,Semantic segmentation models with 500+ pretrained convolutional and transformer-based backbones.,"['computer-vision', 'deeplab-v3-plus', 'deeplabv3', 'dpt', 'fpn', 'image-processing', 'image-segmentation', 'imagenet', 'models', 'pretrained-weights', 'pspnet', 'pytorch', 'segformer', 'segmentation', 'segmentation-models', 'semantic-segmentation', 'transformers', 'unet', 'unet-pytorch', 'unetplusplus']",Python,2019-03-01T16:21:21Z,2025-12-09T08:53:35Z,True,"['examples/dpt_inference_pretrained.ipynb', 'examples/segformer_inference_pretrained.ipynb', 'examples/upernet_inference_pretrained.ipynb', 'segmentation_models_pytorch/encoders/_legacy_pretrained_settings.py', 'segmentation_models_pytorch/utils/train.py']",True,"['.github/workflows/tests.yml', 'misc/generate_test_models.py', 'requirements/test.txt', 'tests/__init__.py', 'tests/base/test_freeze_encoder.py']",86,MIT License
xformers,facebookresearch,https://github.com/facebookresearch/xformers,10167,744,"Hackable and optimized Transformers building blocks, supporting a composable construction.",[],Python,2021-10-13T18:08:50Z,2025-12-09T18:03:33Z,True,['xformers/ops/fmha/merge_training.py'],True,"['.github/gpu_benchmark_diff.py', '.github/run_benchmark_wrapper.py', '.github/workflows/gpu_test_gh.yml', 'requirements-test.txt', 'stubs/torch_stub_tests.py']",366,Other
petals,bigscience-workshop,https://github.com/bigscience-workshop/petals,9851,586,"üå∏ Run LLMs at home, BitTorrent-style. Fine-tuning and inference up to 10x faster than offloading","['bloom', 'chatbot', 'deep-learning', 'distributed-systems', 'falcon', 'gpt', 'guanaco', 'language-models', 'large-language-models', 'llama', 'machine-learning', 'mixtral', 'neural-networks', 'nlp', 'pipeline-parallelism', 'pretrained-models', 'pytorch', 'tensor-parallelism', 'transformer', 'volunteer-computing']",Python,2022-06-12T00:10:27Z,2025-12-09T08:50:16Z,True,"['benchmarks/benchmark_training.py', 'src/petals/client/from_pretrained.py', 'src/petals/server/from_pretrained.py']",True,"['.github/workflows/run-tests.yaml', 'benchmarks/benchmark_forward.py', 'benchmarks/benchmark_inference.py', 'benchmarks/benchmark_training.py', 'tests/bootstrap.id']",111,MIT License
manga-image-translator,zyddnys,https://github.com/zyddnys/manga-image-translator,9000,883,Translate manga/image ‰∏ÄÈîÆÁøªËØëÂêÑÁ±ªÂõæÁâáÂÜÖÊñáÂ≠ó https://cotrans.touhou.ai/ (no longer working),"['anime', 'auto-translation', 'chinese-translation', 'deep-learning', 'image-processing', 'inpainting', 'japanese-translations', 'machine-translation', 'manga', 'neural-network', 'ocr', 'pytorch-implementation', 'text-detection', 'text-detection-recognition', 'transformer']",Python,2021-02-18T03:03:23Z,2025-12-10T03:29:23Z,True,"['training/all-fonts.txt', 'training/inpainting/readme.md', 'training/ocr/readme.md', 'training/ocr/custom_ctc.cc', 'training/ocr/custom_ctc.py']",True,"['pytest.ini', 'test/readme.md', 'test/api_test.html', 'test/conftest.py', 'test/test_render.py']",235,GNU General Public License v3.0
trax,google,https://github.com/google/trax,8294,827,Trax ‚Äî Deep Learning with Clear Code and Speed,"['deep-learning', 'deep-reinforcement-learning', 'jax', 'machine-learning', 'numpy', 'reinforcement-learning', 'transformer']",Python,2019-10-05T15:09:14Z,2025-12-08T16:44:36Z,True,"['trax/data/testdata/c4/en/2.3.0/c4-train.tfrecord-00000-of-00001', 'trax/data/testdata/para_crawl/ende/1.2.0/para_crawl-train.tfrecord-00000-of-00001', 'trax/data/testdata/squad/v1.1/3.0.0/squad-train.tfrecord-00000-of-00001', 'trax/models/reformer/testdata/translate_ende_wmt32k-train-00000-of-00001', 'trax/models/research/testdata/translate_ende_wmt32k-train-00000-of-00001']",True,"['oss_scripts/oss_tests.sh', 'trax/data/inputs_test.py', 'trax/data/testdata/bert_uncased_vocab.txt', 'trax/data/testdata/c4/en/2.3.0/c4-train.tfrecord-00000-of-00001', 'trax/data/testdata/c4/en/2.3.0/c4-validation.tfrecord-00000-of-00001']",125,Apache License 2.0
jukebox,openai,https://github.com/openai/jukebox,8034,1456,"Code for the paper ""Jukebox: A Generative Model for Music""","['audio', 'generative-model', 'music', 'paper', 'pytorch', 'transformer', 'vq-vae']",Python,2020-04-29T17:16:12Z,2025-12-09T13:37:44Z,True,"['jukebox/train.py', 'jukebox/train.py', 'tensorboardx/examples/chainer/extension_logger/train_dcgan.py', 'tensorboardx/examples/chainer/extension_logger/train_dcgan.py', 'tensorboardx/examples/chainer/plain_logger/train_vae.py']",True,"['apex/tests/l0/run_amp/__init__.py', 'apex/tests/l0/run_amp/test_add_param_group.py', 'apex/tests/l0/run_amp/test_basic_casts.py', 'apex/tests/l0/run_amp/test_cache.py', 'apex/tests/l0/run_amp/test_multi_tensor_axpby.py']",207,Other
GPT2-Chinese,Morizeyao,https://github.com/Morizeyao/GPT2-Chinese,7601,1699,"Chinese version of GPT2 training code, using BERT tokenizer.","['chinese', 'gpt-2', 'nlp', 'text-generation', 'transformer']",Python,2019-05-31T02:07:50Z,2025-12-09T14:23:50Z,True,"['scripts/train.sh', 'train.py', 'train.py', 'train.sh']",True,['config/model_config_test.json'],109,MIT License
gpt-neox,EleutherAI,https://github.com/EleutherAI/gpt-neox,7348,1092,"An implementation of model parallel autoregressive transformers on GPUs, based on the Megatron and DeepSpeed libraries","['deepspeed-library', 'gpt-3', 'language-model', 'transformers']",Python,2020-12-22T14:37:54Z,2025-12-10T02:49:53Z,True,"['configs/finetuning_configs/6-9b.yml', 'configs/llama/train_config.yml', 'configs/llama/train_config.yml', 'megatron/tokenizer/train_tokenizer.py', 'megatron/tokenizer/train_tokenizer.py']",True,"['eval.py', 'tests/readme.md', 'tests/__init__.py', 'tests/common.py', 'tests/config/test_setup.yml']",87,Apache License 2.0
BERT-pytorch,codertimo,https://github.com/codertimo/BERT-pytorch,6505,1329,Google AI 2018 BERT pytorch implementation,"['bert', 'language-model', 'nlp', 'pytorch', 'transformer']",Python,2018-10-15T12:58:15Z,2025-12-10T02:25:35Z,True,"['bert/pretrain/__init__.py', 'bert/pretrain/dataset.py', 'bert/pretrain/feature.py', 'bert/pretrain/utils.py', 'scripts/create_pretraining_dataset.py']",True,"['tests/__init__.py', 'tests/test_model.py', 'tests/test_sample.py']",68,Apache License 2.0
ProPainter,sczhou,https://github.com/sczhou/ProPainter,6401,754,[ICCV 2023] ProPainter: Improving Propagation and Transformer for Video Inpainting,"['object-removal', 'video-completion', 'video-inpainting', 'video-outpainting', 'watermark-removal']",Python,2023-09-01T13:11:57Z,2025-12-10T04:30:20Z,True,"['configs/train_flowcomp.json', 'configs/train_flowcomp.json', 'configs/train_propainter.json', 'configs/train_propainter.json', 'core/trainer.py']",True,"['datasets/davis/test.json', 'datasets/youtube-vos/test.json', 'scripts/evaluate_flow_completion.py', 'scripts/evaluate_propainter.py', 'web-demos/hugging_face/test_sample/test-sample0.mp4']",73,Other
x-transformers,lucidrains,https://github.com/lucidrains/x-transformers,5710,497,A concise but complete full-attention transformer with a set of promising experimental features from various papers,"['artificial-intelligence', 'attention-mechanism', 'deep-learning', 'transformers']",Python,2020-10-24T22:13:25Z,2025-12-08T08:19:07Z,True,"['train_belief_state.py', 'train_belief_state.py', 'train_copy.py', 'train_copy.py', 'train_entropy_tokenizer.py']",True,"['.github/workflows/python-test.yaml', 'tests/test_x_transformers.py']",71,MIT License
Sana,NVlabs,https://github.com/NVlabs/Sana,4781,313,SANA: Efficient High-Resolution Image Synthesis with Linear Diffusion Transformer,"['diffusion', 'dit', 'pytorch', 'sana', 'text-to-image-generation', 'transformers']",Python,2024-10-11T20:19:45Z,2025-12-10T04:06:25Z,True,"['asset/samples/longsana_train.txt', 'diffusion/longsana/pipeline/sana_switch_training_pipeline.py', 'diffusion/longsana/pipeline/sana_switch_training_pipeline.py', 'diffusion/longsana/pipeline/sana_training_pipeline.py', 'diffusion/longsana/pipeline/sana_training_pipeline.py']",True,"['scripts/inference_geneval.py', 'scripts/inference_sana_sprint_geneval.py', 'tests/bash/entry.sh', 'tests/bash/inference/test_inference.sh', 'tests/bash/setup_test_data.sh']",92,Apache License 2.0
transformerlab-app,transformerlab,https://github.com/transformerlab/transformerlab-app,4581,465,"Open Source Application for Advanced LLM + Diffusion Engineering: interact, train, fine-tune, and evaluate large language models on your own computer.","['diffusion', 'diffusion-models', 'electron', 'llama', 'llms', 'lora', 'mlx', 'rlhf', 'stability-diffusion', 'transformers']",Python,2023-12-24T22:09:14Z,2025-12-09T23:54:28Z,True,"['api/scripts/xml-rpc-client-example/train_example.py', 'api/scripts/xml-rpc-client-example/train_example.py', 'api/scripts/xml-rpc-client-example/training_client_example.py', 'api/scripts/xml-rpc-client-example/training_client_example.py', 'api/test/api/test_train.py']",True,"['.github/workflows/pytest-sdk.yml', '.github/workflows/pytest-server-test-macos.yml', '.github/workflows/pytest-server-test.yml', '.github/workflows/pytest.yml', '.github/workflows/test.yml']",68,GNU Affero General Public License v3.0
RT-DETR,lyuwenyu,https://github.com/lyuwenyu/RT-DETR,4557,534,"[CVPR 2024] Official RT-DETR (RTDETR paddle pytorch), Real-Time DEtection TRansformer, DETRs Beat YOLOs on Real-time Object Detection. üî• üî• üî• ","['rtdetr', 'rtdetrv2']",Python,2023-05-10T06:35:56Z,2025-12-10T03:19:05Z,True,"['rtdetr_paddle/ppdet/engine/trainer.py', 'rtdetr_paddle/tools/train.py', 'rtdetr_paddle/tools/train.py', 'rtdetr_pytorch/tools/train.py', 'rtdetr_pytorch/tools/train.py']",True,"['rtdetr_paddle/ppdet/modeling/transformers/ext_op/test_ms_deformable_attn_op.py', 'rtdetr_paddle/tools/eval.py', 'rtdetr_pytorch/src/data/coco/coco_eval.py', 'rtdetr_pytorch/src/nn/backbone/test_resnet.py', 'rtdetrv2_pytorch/src/data/dataset/coco_eval.py']",402,Apache License 2.0
Efficient-AI-Backbones,huawei-noah,https://github.com/huawei-noah/Efficient-AI-Backbones,4353,735,"Efficient AI Backbones including GhostNet, TNT and MLP, developed by Huawei Noah's Ark Lab.","['convolutional-neural-networks', 'efficient-inference', 'ghostnet', 'imagenet', 'model-compression', 'pretrained-models', 'pytorch', 'tensorflow', 'transformer', 'vision-transformer']",Python,2019-11-16T14:21:35Z,2025-12-09T15:26:57Z,True,"['augvit_pytorch/train.py', 'augvit_pytorch/train.py', 'cmt_pytorch/train.py', 'cmt_pytorch/train.py', 'ghostnetv2_pytorch/train.py']",True,"['snnmlp_pytorch/train_scripts/test.sh', 'tinynet_pytorch/eval.py']",94,Unknown
transformer,hyunwoongko,https://github.com/hyunwoongko/transformer,4308,612,"Transformer: PyTorch Implementation of ""Attention Is All You Need""","['attention', 'dataset', 'pytorch', 'transformer']",Python,2019-10-15T10:36:00Z,2025-12-09T16:18:58Z,True,"['saved/transformer-base/train.txt', 'saved/transformer-base/train_result.jpg', 'saved/transformer-base/train_result.jpg', 'train.py', 'train.py']",True,['saved/transformer-base/test.txt'],18,Unknown
transformer-xl,kimiyoung,https://github.com/kimiyoung/transformer-xl,3680,765,,[],Python,2019-01-08T12:20:24Z,2025-11-27T05:58:07Z,True,"['pytorch/train.py', 'pytorch/train.py', 'tf/train.py', 'tf/train.py', 'tf/train_gpu.py']",True,['pytorch/eval.py'],97,Apache License 2.0
HRNet-Semantic-Segmentation,HRNet,https://github.com/HRNet/HRNet-Semantic-Segmentation,3305,697,The OCR approach is rephrased as Segmentation Transformer: https://arxiv.org/abs/1909.11065. This is an official implementation of semantic segmentation for HRNet. https://arxiv.org/abs/1908.07919,"['cityscapes', 'high-resolution', 'high-resolution-net', 'hrnets', 'lip', 'pascal-context', 'segmentation', 'segmentation-transformer', 'semantic-segmentation', 'transformer']",Python,2019-04-09T13:24:09Z,2025-12-08T14:11:52Z,True,"['data/list/cityscapes/train.lst', 'data/list/cityscapes/trainval.lst', 'data/list/lip/trainlist.txt', 'experiments/cityscapes/seg_hrnet_w48_train_512x1024_sgd_lr1e-2_wd5e-4_bs_12_epoch484.yaml', 'experiments/cityscapes/seg_hrnet_w48_train_512x1024_sgd_lr1e-2_wd5e-4_bs_12_epoch484.yaml']",True,"['data/list/cityscapes/test.lst', 'data/list/lip/testvallist.txt', 'tools/test.py']",163,Other
SwanLab,SwanHubX,https://github.com/SwanHubX/SwanLab,3230,176,"‚ö°Ô∏èSwanLab - an open-source, modern-design AI training tracking and visualization tool. Supports Cloud / Self-hosted use. Integrated with PyTorch / Transformers / LLaMA Factory / veRL/ Swift / Ultralytics / MMEngine / Keras etc.","['data-science', 'deep-learning', 'logging', 'machine-learning', 'mlops', 'model-versioning', 'python', 'pytorch', 'tensorboard', 'tensorflow', 'tracking', 'transformers', 'visualization']",Python,2023-11-24T08:54:45Z,2025-12-10T01:38:33Z,True,"['test/integration/accelerate/accelerate_train.py', 'test/integration/accelerate/accelerate_train.py', 'test/integration/fastai/fastai_train.py', 'test/integration/fastai/fastai_train.py', 'test/integration/keras/keras_train.py']",True,"['.github/workflows/test-core.yml', '.github/workflows/test-when-pr.yml', 'core/internal/api/parse_test.go', 'test/config/config.json', 'test/config/load.yaml']",52,Apache License 2.0
SegFormer,NVlabs,https://github.com/NVlabs/SegFormer,3228,412,Official PyTorch implementation of SegFormer,"['ade20k', 'cityscapes', 'semantic-segmentation', 'transformer']",Python,2021-06-11T17:22:07Z,2025-12-10T02:08:37Z,True,"['docs/train.md', 'docs/tutorials/training_tricks.md', 'docs/tutorials/training_tricks.md', 'mmseg/apis/train.py', 'mmseg/apis/train.py']",True,"['mmseg/apis/test.py', 'mmseg/datasets/pipelines/test_time_aug.py', 'pytest.ini', 'requirements/tests.txt', 'tests/test_config.py']",109,Other
Mask2Former,facebookresearch,https://github.com/facebookresearch/Mask2Former,3133,483,"Code release for ""Masked-attention Mask Transformer for Universal Image Segmentation""",[],Python,2021-11-24T16:00:44Z,2025-12-10T02:20:30Z,True,"['tools/convert-pretrained-swin-model-to-d2.py', 'train_net.py', 'train_net.py', 'train_net_video.py', 'train_net_video.py']",True,"['mask2former/modeling/pixel_decoder/ops/test.py', 'mask2former/test_time_augmentation.py', 'mask2former_video/data_video/datasets/ytvis_api/ytvoseval.py', 'mask2former_video/data_video/ytvis_eval.py', 'tools/evaluate_coco_boundary_ap.py']",164,MIT License
yolov7_d2,lucasjinreal,https://github.com/lucasjinreal/yolov7_d2,3124,476,"üî•üî•üî•üî• (Earlier YOLOv7 not official one) YOLO with Transformers and Instance Segmentation, with TensorRT acceleration! üî•üî•üî•","['detection', 'detextron2', 'detr', 'face', 'instance-segmentation', 'object-detection', 'onnx', 'tensorrt', 'transformers', 'yolo', 'yolov6', 'yolov7', 'yolox']",Python,2021-06-23T11:35:35Z,2025-12-10T00:54:53Z,True,"['configs/common/train.py', 'configs/common/train.py', 'tools/lazyconfig_train_net.py', 'tools/lazyconfig_train_net.py', 'tools/train_detr.py']",True,"['deploy/quant_fx/fx_ptq_test.py', 'deploy/quant_fx/qt_mq_test.py', 'deploy/quant_fx/qt_q_test.py', 'deploy/quant_fx/quant_ptq_test.py', 'deploy/quant_fx/test.py']",69,GNU General Public License v3.0
TransUNet,Beckschen,https://github.com/Beckschen/TransUNet,3020,567,"This repository includes the official project of TransUNet, presented in our paper: TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation.",[],Python,2021-02-08T06:12:54Z,2025-12-09T15:00:44Z,True,"['lists/lists_synapse/train.txt', 'train.py', 'train.py', 'trainer.py']",True,"['lists/lists_synapse/test_vol.txt', 'test.py']",137,Apache License 2.0
table-transformer,microsoft,https://github.com/microsoft/table-transformer,2798,304,Table Transformer (TATR) is a deep learning model for extracting tables from unstructured documents (PDFs and images). This is also the official repository for the PubTables-1M dataset and GriTS evaluation metric.,"['table-detection', 'table-extraction', 'table-functional-analysis', 'table-structure-recognition']",Python,2021-05-17T19:01:34Z,2025-12-10T02:18:20Z,True,"['detr/d2/train_net.py', 'detr/d2/train_net.py']",True,"['detr/datasets/coco_eval.py', 'detr/datasets/panoptic_eval.py', 'detr/test_all.py', 'src/eval.py']",106,MIT License
decision-transformer,kzl,https://github.com/kzl/decision-transformer,2718,504,Official codebase for Decision Transformer: Reinforcement Learning via Sequence Modeling.,[],Python,2021-06-02T09:35:37Z,2025-12-09T16:52:43Z,True,"['atari/mingpt/trainer_atari.py', 'gym/decision_transformer/training/act_trainer.py', 'gym/decision_transformer/training/seq_trainer.py', 'gym/decision_transformer/training/trainer.py']",True,['gym/decision_transformer/evaluation/evaluate_episodes.py'],37,MIT License
mPLUG-Owl,X-PLUG,https://github.com/X-PLUG/mPLUG-Owl,2536,191,mPLUG-Owl: The Powerful Multi-modal Large Language Model  Family,"['alpaca', 'chatbot', 'chatgpt', 'damo', 'dialogue', 'gpt', 'gpt4', 'gpt4-api', 'huggingface', 'instruction-tuning', 'large-language-models', 'llama', 'mplug', 'mplug-owl', 'multimodal', 'pretraining', 'pytorch', 'transformer', 'video', 'visual-recognition']",Python,2023-04-25T02:31:04Z,2025-12-10T03:32:31Z,True,"['mplug-owl/pipeline/train.py', 'mplug-owl/pipeline/train.py', 'mplug-owl/scripts/train_it.sh', 'mplug-owl/scripts/train_it.sh', 'mplug-owl/scripts/train_it_wo_lora.sh']",True,"['mplug-owl2/mplug_owl2/evaluate/evaluate_caption.py', 'mplug-owl2/mplug_owl2/evaluate/evaluate_mmbench.py', 'mplug-owl2/mplug_owl2/evaluate/evaluate_mme.py', 'mplug-owl2/mplug_owl2/evaluate/evaluate_mmmu.py', 'mplug-owl2/mplug_owl2/evaluate/evaluate_vqa.py']",100,MIT License
EasyLM,young-geng,https://github.com/young-geng/EasyLM,2502,258,"Large language models (LLMs) made easy, EasyLM is a one stop solution for pre-training, finetuning, evaluating and serving LLMs in JAX/Flax.","['chatbot', 'deep-learning', 'flax', 'jax', 'language-model', 'large-language-models', 'llama', 'natural-language-processing', 'transformer']",Python,2022-11-22T12:55:20Z,2025-11-30T01:17:20Z,True,"['easylm/models/llama/llama_train.py', 'easylm/models/llama/llama_train.py', 'examples/pretrain_llama_7b.sh', 'examples/pretrain_llama_7b.sh']",True,['easylm/scripts/benchmark_attention.py'],31,Apache License 2.0
DialoGPT,microsoft,https://github.com/microsoft/DialoGPT,2411,350,Large-scale pretraining for dialogue,"['data-processing', 'dialogpt', 'dialogue', 'gpt-2', 'machine-learning', 'pytorch', 'text-data', 'text-generation', 'transformer']",Python,2019-08-29T21:07:46Z,2025-11-25T19:28:18Z,True,"['lsp_train.py', 'lsp_train.py', 'data/train_raw.tsv', 'data/train_raw.tsv', 'gpt2_training/distributed.py']",True,"['dstc/batch_eval.py', 'dstc/data/processed/test_real.keys.txt', 'pycocoevalcap/eval.py', 'reddit_extractor/data/keys-test.gz', 'reddit_extractor/data/test-multi-refs-ids.txt']",64,MIT License
llm-compressor,vllm-project,https://github.com/vllm-project/llm-compressor,2367,312,Transformers-compatible library for applying various compression algorithms to LLMs for optimized deployment with vLLM,"['compression', 'quantization', 'sparsity']",Python,2024-06-20T20:13:34Z,2025-12-10T04:48:23Z,True,"['examples/finetuning/configure_fsdp.md', 'examples/finetuning/example_alternating_recipe.yaml', 'examples/finetuning/example_fsdp_config.yaml', 'examples/finetuning/example_single_gpu_config.yaml']",True,"['.github/workflows/test-check-transformers.yaml', '.github/workflows/test-check.yaml', 'tests/__init__.py', 'tests/e2e/__init__.py', 'tests/e2e/e2e_utils.py']",120,Apache License 2.0
Swin-Unet,HuCaoFighting,https://github.com/HuCaoFighting/Swin-Unet,2251,356,"[ECCVW 2022] The codes for the work ""Swin-Unet: Unet-like Pure Transformer for Medical Image Segmentation""",[],Python,2021-05-03T07:37:40Z,2025-12-10T02:49:26Z,True,"['lists/lists_synapse/train.txt', 'train.py', 'train.py', 'train.sh', 'trainer.py']",True,"['lists/lists_synapse/test_vol.txt', 'test.py', 'test.sh']",88,Unknown
EasyAnimate,aigc-apps,https://github.com/aigc-apps/EasyAnimate,2239,178,üì∫ An End-to-End Solution for High-Resolution and Long Video Generation Based on Transformer Diffusion,[],Python,2024-04-11T08:52:50Z,2025-12-08T06:21:23Z,True,"['easyanimate/reward/mps/trainer/models/base_model.py', 'easyanimate/reward/mps/trainer/models/clip_model.py', 'easyanimate/reward/mps/trainer/models/cross_modeling.py', 'easyanimate/video_caption/filter_meta_train.py', 'easyanimate/video_caption/filter_meta_train.py']",True,['easyanimate/vae/ldm/modules/image_degradation/utils/test.png'],96,Apache License 2.0
longformer,allenai,https://github.com/allenai/longformer,2176,288,Longformer: The Long-Document Transformer,[],Python,2020-03-31T21:07:29Z,2025-12-08T13:51:15Z,True,"['scripts/pretrain.py', 'scripts/pretrain.py']",True,"['scripts/test_tpu.py', 'tests/test_integration.py', 'tests/test_readme.py', 'tests/test_sliding_chunks.py', 'tests/test_var_global_attn.py']",139,Apache License 2.0
intel-extension-for-transformers,intel,https://github.com/intel/intel-extension-for-transformers,2169,216,‚ö° Build your chatbot within minutes on your favorite device; offer SOTA compression techniques for LLMs; run LLMs efficiently on Intel Platforms‚ö°,"['4-bits', 'autoround', 'chatbot', 'chatpdf', 'gaudi3', 'habana', 'intel-optimized-llamacpp', 'large-language-model', 'llm-cpu', 'llm-inference', 'neural-chat', 'neural-chat-7b', 'rag', 'retrieval', 'speculative-decoding', 'streamingllm']",Python,2022-11-11T05:32:27Z,2025-12-02T02:53:29Z,True,"['.github/workflows/chatbot-finetune-mpt-7b-chat-hpu.yml', '.github/workflows/chatbot-finetune-mpt-7b-chat.yml', '.github/workflows/chatbot_finetuning.yml', 'docs/api_doc/optimization/trainer.rst', 'docs/tutorials/pytorch/question-answering/bert-large-uncased-whole-word-masking-finetuned-squad.ipynb']",True,"['.github/workflows/chatbot-test.yml', '.github/workflows/deploy-test.yml', '.github/workflows/docker/unittest.dockerfile', '.github/workflows/llm-test.yml', '.github/workflows/optimize-test.yml']",56,Apache License 2.0
MambaVision,NVlabs,https://github.com/NVlabs/MambaVision,1933,113,[CVPR 2025] Official PyTorch Implementation of MambaVision: A Hybrid Mamba-Transformer Vision Backbone,"['deep-learning', 'foundation-models', 'huggingface-transformers', 'hybrid-models', 'image-classification', 'instance-segmentation', 'mamba', 'object-detection', 'self-attention', 'semantic-segmentation', 'transformers', 'vision-transformer', 'visual-recognition']",Python,2024-06-10T17:46:08Z,2025-12-10T04:07:27Z,True,"['mambavision/train.py', 'mambavision/train.py', 'mambavision/train.sh', 'object_detection/tools/train.py', 'object_detection/tools/train.py']",True,"['mambavision/dummy_test.py', 'object_detection/tools/analysis_tools/robustness_eval.py', 'object_detection/tools/analysis_tools/test_robustness.py', 'object_detection/tools/deployment/test_torchserver.py', 'object_detection/tools/misc/gen_coco_panoptic_test_info.py']",25,Other
BitNet,kyegomez,https://github.com/kyegomez/BitNet,1894,169,"Implementation of ""BitNet: Scaling 1-bit Transformers for Large Language Models"" in pytorch","['artificial-intelligence', 'deep-neural-networks', 'deeplearning', 'gpt4', 'machine-learning', 'multimodal', 'multimodal-deep-learning']",Python,2023-10-18T16:19:06Z,2025-12-08T16:44:42Z,True,"['train.py', 'train.py']",True,"['.github/workflows/docs_test.yml', '.github/workflows/run_test.yml', '.github/workflows/test.yml', '.github/workflows/testing.yml', '.github/workflows/unit-test.yml']",5,MIT License
PVT,whai362,https://github.com/whai362/PVT,1867,254,Official implementation of PVT series,"['backbone', 'detection', 'pvt', 'pvtv2', 'segmentation', 'transformer']",Python,2021-02-24T02:01:37Z,2025-11-30T10:58:46Z,True,"['detection/dist_train.sh', 'detection/train.py', 'detection/train.py', 'dist_train.sh']",True,"['detection/dist_test.sh', 'detection/test.py']",40,Apache License 2.0
ViTPose,ViTAE-Transformer,https://github.com/ViTAE-Transformer/ViTPose,1853,230,"The official repo for [NeurIPS'22] ""ViTPose: Simple Vision Transformer Baselines for Human Pose Estimation"" and [TPAMI'23] ""ViTPose++: Vision Transformer for Generic Body Pose Estimation""","['deep-learning', 'distillation', 'mae', 'pose-estimation', 'pytorch', 'self-supervised-learning', 'vision-transformer']",Python,2022-04-27T01:09:19Z,2025-12-09T03:09:56Z,True,"['docs/en/tutorials/1_finetune.md', 'docs/zh_cn/tutorials/1_finetune.md', 'mmpose/apis/train.py', 'mmpose/apis/train.py', 'tests/data/jhmdb/goalkeeper_training_day_@_7_catch_f_cm_np1_ri_med_0/00001.png']",True,"['mmpose/apis/test.py', 'mmpose/core/evaluation/bottom_up_eval.py', 'mmpose/core/evaluation/mesh_eval.py', 'mmpose/core/evaluation/pose3d_eval.py', 'mmpose/core/evaluation/top_down_eval.py']",106,Apache License 2.0
OminiControl,Yuanshi9815,https://github.com/Yuanshi9815/OminiControl,1848,140,[ICCV 2025 Highlight] OminiControl: Minimal and Universal Control for Diffusion Transformer,[],Python,2024-11-17T08:53:18Z,2025-12-09T10:47:30Z,True,"['omini/train_flux/train_custom.py', 'omini/train_flux/train_custom.py', 'omini/train_flux/train_multi_condition.py', 'omini/train_flux/train_multi_condition.py', 'omini/train_flux/train_spatial_alignment.py']",True,"['assets/test_in.jpg', 'assets/test_out.jpg']",62,Apache License 2.0
Cream,microsoft,https://github.com/microsoft/Cream,1811,239,This is a collection of our NAS and Vision Transformer work.,"['automl', 'efficiency', 'knowledge-distillation', 'nas', 'rpe', 'vision-transformer', 'vit-compression']",Python,2020-10-12T09:30:03Z,2025-12-09T08:03:01Z,True,"['autoformer/supernet_train.py', 'autoformer/supernet_train.py', 'cdarts/cdarts/retrain.py', 'cdarts/cdarts/retrain.py', 'cdarts/cdarts_detection/mmdet/apis/train.py']",True,"['cdarts/cdarts/test.py', 'cdarts/cdarts_detection/mmcv/runner/parallel_test.py', 'cdarts/cdarts_detection/mmdet/datasets/pipelines/test_aug.py', 'cdarts/cdarts_detection/mmdet/models/detectors/test_mixins.py', 'cdarts/cdarts_detection/test.py']",34,MIT License
Show-o,showlab,https://github.com/showlab/Show-o,1809,80,"[ICLR & NeurIPS 2025] Repository for Show-o series, One Single Transformer to Unify Multimodal Understanding and Generation.","['diffusion-models', 'large-language-models', 'multimodal']",Python,2024-08-09T05:26:23Z,2025-12-10T03:16:40Z,True,"['configs/showo_pretraining_stage1.yaml', 'configs/showo_pretraining_stage1.yaml', 'configs/showo_pretraining_stage2.yaml', 'configs/showo_pretraining_stage2.yaml', 'configs/showo_pretraining_stage3.yaml']",True,['show-o2/evaluation/inference_geneval.py'],62,Apache License 2.0
CogView,zai-org,https://github.com/zai-org/CogView,1794,179,"Text-to-Image generation. The repo for NeurIPS 2021 paper ""CogView: Mastering Text-to-Image Generation via Transformers"".","['pretrained-models', 'pytorch', 'text-to-image', 'transformers']",Python,2021-05-25T14:48:31Z,2025-12-09T13:45:40Z,True,"['finetune/__init__.py', 'pretrain_gpt2.py', 'pretrain_gpt2.py', 'pretrained/chinese_sentencepiece/cog-pretrain.model', 'pretrained/chinese_sentencepiece/cog-pretrain.vocab']",True,['test_lmdb.py'],19,Apache License 2.0
FluxMusic,feizc,https://github.com/feizc/FluxMusic,1712,128,Text-to-Music Generation with Rectified Flow Transformers,[],Python,2024-08-06T09:41:07Z,2025-12-03T11:31:50Z,True,"['audioldm2/clap/open_clip/pretrained.py', 'audioldm2/clap/training/__init__.py', 'audioldm2/clap/training/__pycache__/__init__.cpython-310.pyc', 'audioldm2/clap/training/__pycache__/data.cpython-310.pyc', 'audioldm2/clap/training/audioset_textmap.npy']",True,['test.py'],19,Other
EasyControl,Xiaojiu-z,https://github.com/Xiaojiu-z/EasyControl,1703,127,"Implementation of ""EasyControl: Adding Efficient and Flexible Control for Diffusion Transformer""(ICCV2025)",[],Python,2025-03-06T09:33:30Z,2025-12-09T03:36:33Z,True,"['train/default_config.yaml', 'train/examples/openpose_data/1.png', 'train/examples/openpose_data/2.png', 'train/examples/pose.jsonl', 'train/examples/style.jsonl']",True,"['test_imgs/canny.png', 'test_imgs/depth.png', 'test_imgs/ghibli.png', 'test_imgs/inpainting.png', 'test_imgs/openpose.png']",23,Apache License 2.0
TransGAN,VITA-Group,https://github.com/VITA-Group/TransGAN,1688,204,"[NeurIPS‚Äò2021] ""TransGAN: Two Pure Transformers Can Make One Strong GAN, and That Can Scale Up"", Yifan Jiang, Shiyu Chang, Zhangyang Wang","['gan', 'pytorch', 'transformer', 'transformer-encoder', 'transformer-models']",Python,2021-02-10T18:11:54Z,2025-11-25T10:54:36Z,True,"['exps/celeba_hq_256_train.py', 'exps/celeba_hq_256_train.py', 'exps/church_256_train.py', 'exps/church_256_train.py', 'exps/cifar_train.py']",True,"['exps/celeba_hq_256_test.py', 'exps/cifar_test.py', 'test.py']",15,Other
titans-pytorch,lucidrains,https://github.com/lucidrains/titans-pytorch,1655,158,"Unofficial implementation of Titans, SOTA memory for transformers, in Pytorch","['artificial-intelligence', 'deep-learning', 'long-term-memory', 'test-time-training']",Python,2025-01-08T15:26:27Z,2025-12-10T05:01:11Z,True,"['train_implicit_mlp_attn.py', 'train_implicit_mlp_attn.py', 'train_mac.py', 'train_mac.py']",True,"['.github/workflows/test.yaml', 'tests/test_titans.py']",31,MIT License
MetaTransformer,invictus717,https://github.com/invictus717/MetaTransformer,1644,117,Meta-Transformer for Unified Multimodal Learning,"['artificial-intelligence', 'computer-vision', 'foundationmodel', 'machine-learning', 'multimedia', 'multimodal', 'transformers']",Python,2023-07-08T12:40:54Z,2025-12-05T03:19:08Z,True,"['audio/src/traintest.py', 'autonomousdriving/data/kitti/imagesets/train.txt', 'autonomousdriving/data/lyft/imagesets/train.txt', 'autonomousdriving/data/once/imagesets/train.txt', 'autonomousdriving/data/waymo/imagesets/train.txt']",True,"['audio/src/traintest.py', 'autonomousdriving/data/kitti/imagesets/test.txt', 'autonomousdriving/data/lyft/imagesets/test.txt', 'autonomousdriving/data/once/imagesets/test.txt', 'autonomousdriving/pcdet/datasets/kitti/kitti_eval.py']",4,Apache License 2.0
robotics_transformer,google-research,https://github.com/google-research/robotics_transformer,1642,189,,[],Python,2022-12-05T00:39:23Z,2025-12-10T02:30:22Z,True,"['film_efficientnet/pretrained_efficientnet_encoder.py', 'film_efficientnet/pretrained_efficientnet_encoder_test.py', 'trained_checkpoints/rt1main/assets/metadata.textproto', 'trained_checkpoints/rt1main/checkpoint', 'trained_checkpoints/rt1main/ckpt-424760.data-00000-of-00001']",True,"['film_efficientnet/film_conditioning_layer_test.py', 'film_efficientnet/film_efficientnet_encoder_test.py', 'film_efficientnet/preprocessors_test.py', 'film_efficientnet/pretrained_efficientnet_encoder_test.py', 'sequence_agent_test.py']",23,Apache License 2.0
safe-rlhf,PKU-Alignment,https://github.com/PKU-Alignment/safe-rlhf,1563,128,Safe RLHF: Constrained Value Alignment via Safe Reinforcement Learning from Human Feedback,"['ai-safety', 'alpaca', 'beaver', 'datasets', 'deepspeed', 'gpt', 'large-language-models', 'llama', 'llm', 'llms', 'reinforcement-learning', 'reinforcement-learning-from-human-feedback', 'rlhf', 'safe-reinforcement-learning', 'safe-reinforcement-learning-from-human-feedback', 'safe-rlhf', 'safety', 'transformer', 'transformers', 'vicuna']",Python,2023-05-15T11:47:08Z,2025-12-08T09:07:03Z,True,"['safe_rlhf/algorithms/dpo/trainer.py', 'safe_rlhf/algorithms/ppo/trainer.py', 'safe_rlhf/algorithms/ppo_lag/trainer.py', 'safe_rlhf/algorithms/ppo_reward_shaping/trainer.py', 'safe_rlhf/configs/ds_train_config_template.json']",True,"['safe_rlhf/evaluate/bigbench/eval.py', 'safe_rlhf/evaluate/gpt4/eval.py']",18,Apache License 2.0
RoboticsDiffusionTransformer,thu-ml,https://github.com/thu-ml/RoboticsDiffusionTransformer,1549,145,RDT-1B: a Diffusion Foundation Model for Bimanual Manipulation,[],Python,2024-10-07T10:08:30Z,2025-12-09T06:06:29Z,True,"['configs/finetune_datasets.json', 'configs/finetune_sample_weights.json', 'configs/pretrain_datasets.json', 'configs/pretrain_datasets.json', 'configs/pretrain_sample_weights.json']",True,['docs/test_6drot.py'],40,MIT License
4D-Humans,shubham-goel,https://github.com/shubham-goel/4D-Humans,1486,143,4DHumans: Reconstructing and Tracking Humans with Transformers,['3d-reconstruction'],Python,2023-05-31T21:05:33Z,2025-12-10T00:22:26Z,True,"['fetch_training_data.sh', 'fetch_training_data.sh', 'hmr2/configs_hydra/train.yaml', 'hmr2/configs_hydra/trainer/cpu.yaml', 'hmr2/configs_hydra/trainer/ddp.yaml']",True,['eval.py'],35,MIT License
CodeTF,salesforce,https://github.com/salesforce/CodeTF,1479,97,CodeTF: One-stop Transformer Library for State-of-the-art Code LLM,"['ai4code', 'ai4se', 'code-generation', 'code-intelligence', 'code-learning-datasets', 'code-representation-learning', 'code-understanding', 'human-eval', 'multilingual-parsers', 'transformers', 'tree-sitter']",Python,2023-05-02T05:05:27Z,2025-11-25T10:39:34Z,True,"['codetf/configs/training/causal_lm.yaml', 'codetf/configs/training/codet5.yaml', 'codetf/trainer/base_trainer.py', 'codetf/trainer/causal_lm_trainer.py', 'codetf/trainer/codet5_trainer.py']",True,"['test_code_utilities/__init__.py', 'test_code_utilities/test_extract_code_attributes.py', 'test_code_utilities/test_parse_code.py', 'test_code_utilities/test_remove_comments.py', 'test_code_utilities/test_variable_renaming.py']",28,Apache License 2.0
long_llama,CStanKonrad,https://github.com/CStanKonrad/long_llama,1463,85,LongLLaMA is a large language model capable of handling long contexts. It is based on OpenLLaMA and fine-tuned with the Focused Transformer (FoT) method.,[],Python,2023-07-06T14:54:15Z,2025-11-21T13:13:43Z,True,"['fot_continued_pretraining/easylm/__init__.py', 'fot_continued_pretraining/easylm/bpt.py', 'fot_continued_pretraining/easylm/checkpoint.py', 'fot_continued_pretraining/easylm/data.py', 'fot_continued_pretraining/easylm/jax_utils.py']",True,['fot_continued_pretraining/configs/setup_test.json'],18,Apache License 2.0
octo,octo-models,https://github.com/octo-models/octo,1455,240,Octo is a transformer-based robot policy trained on a diverse mix of 800k robot trajectories.,[],Python,2023-12-13T09:58:56Z,2025-12-10T02:38:23Z,True,"['examples/01_inference_pretrained.ipynb', 'examples/02_finetune_new_observation_action.py', 'examples/03_eval_finetuned.py', 'examples/04_eval_finetuned_on_robot.py', 'octo/utils/train_callbacks.py']",True,"['tests/debug_config.py', 'tests/debug_dataset/bridge_dataset/1.0.0/bridge_dataset-train.tfrecord-00000-of-00001', 'tests/debug_dataset/bridge_dataset/1.0.0/bridge_dataset-val.tfrecord-00000-of-00001', 'tests/debug_dataset/bridge_dataset/1.0.0/dataset_info.json', 'tests/debug_dataset/bridge_dataset/1.0.0/features.json']",92,MIT License
ViT-Adapter,czczup,https://github.com/czczup/ViT-Adapter,1447,151,[ICLR 2023 Spotlight] Vision Transformer Adapter for Dense Predictions,"['adapter', 'object-detection', 'semantic-segmentation', 'vision-transformer']",Python,2022-05-16T17:32:59Z,2025-12-07T21:42:16Z,True,"['detection/dist_train.sh', 'detection/slurm_train.sh', 'detection/train.py', 'detection/train.py', 'segmentation/dist_train.sh']",True,"['detection/dist_test.sh', 'detection/ops/test.py', 'detection/slurm_test.sh', 'detection/test.py', 'segmentation/dist_test.sh']",81,Other
MapTR,hustvl,https://github.com/hustvl/MapTR,1427,229,[ICLR'23 Spotlight & ECCV'24 & IJCV'24] MapTR: Structured Modeling and Learning for Online Vectorized HD Map Construction,"['autonomous-driving', 'bev', 'end-to-end', 'iclr2023', 'online-hdmap-construction', 'real-time', 'shape-representation', 'transformer', 'vectorized-hdmap']",Python,2022-07-28T02:20:43Z,2025-12-09T11:14:04Z,True,"['docs/train_eval.md', 'docs/train_eval.md', 'mmdetection3d/.dev_scripts/train_benchmark.sh', 'mmdetection3d/.dev_scripts/train_benchmark.sh', 'mmdetection3d/configs/fcos3d/fcos3d_r101_caffe_fpn_gn-head_dcn_2x8_1x_nus-mono3d_finetune.py']",True,"['mmdetection3d/.dev_scripts/gen_benchmark_script.py', 'mmdetection3d/.dev_scripts/test_benchmark.sh', 'mmdetection3d/mmdet3d/apis/test.py', 'mmdetection3d/mmdet3d/core/evaluation/indoor_eval.py', 'mmdetection3d/mmdet3d/core/evaluation/kitti_utils/eval.py']",134,MIT License
Megatron-DeepSpeed,bigscience-workshop,https://github.com/bigscience-workshop/Megatron-DeepSpeed,1426,228,"Ongoing research training transformer language models at scale, including: BERT & GPT-2",[],Python,2021-07-02T17:40:35Z,2025-11-26T06:51:29Z,True,"['examples/curriculum_learning/pretrain_gpt_cl.sh', 'examples/curriculum_learning/pretrain_gpt_cl.sh', 'examples/finetune_mnli_distributed.sh', 'examples/finetune_race_distributed.sh', 'examples/pretrain_bert.sh']",True,"['examples/evaluate_ict_zeroshot_nq.sh', 'examples/evaluate_zeroshot_gpt.sh', 'megatron/data/test/test_indexed_dataset.py', 'megatron/data/test/test_preprocess_data.sh', 'megatron/fused_kernels/tests/__init__.py']",123,Other
spacy-transformers,explosion,https://github.com/explosion/spacy-transformers,1402,176,"üõ∏ Use pretrained transformers like BERT, XLNet and GPT-2 in spaCy","['bert', 'google', 'gpt-2', 'huggingface', 'language-model', 'machine-learning', 'natural-language-processing', 'natural-language-understanding', 'nlp', 'openai', 'pytorch', 'pytorch-model', 'spacy', 'spacy-extension', 'spacy-pipeline', 'transfer-learning', 'xlnet']",Python,2019-07-26T19:12:34Z,2025-11-19T10:46:06Z,True,['build-constraints.txt'],True,"['.github/workflows/tests.yml', 'spacy_transformers/tests/__init__.py', 'spacy_transformers/tests/enable_gpu.py', 'spacy_transformers/tests/regression/__init__.py', 'spacy_transformers/tests/regression/test_spacy_issue6401.py']",0,MIT License
gansformer,dorarad,https://github.com/dorarad/gansformer,1341,151,Generative Adversarial Transformers,"['attention', 'compositionality', 'gans', 'generative-adversarial-networks', 'image-generation', 'scene-generation', 'transformers']",Python,2021-03-01T13:39:07Z,2025-11-20T17:21:05Z,True,"['pretrained_networks.py', 'pytorch_version/torch_utils/training_stats.py', 'pytorch_version/torch_utils/training_stats.py', 'pytorch_version/training/__init__.py', 'pytorch_version/training/dataset.py']",True,['test_nvcc.cu'],15,MIT License
Retinexformer,caiyuanhao1998,https://github.com/caiyuanhao1998/Retinexformer,1327,109,"""Retinexformer: One-stage Retinex-based Transformer for Low-light Image Enhancement"" (ICCV 2023) & (NTIRE 2024 Runner-Up)","['basicsr', 'detection', 'iccv2023', 'image-restoration', 'low-light-enhance', 'low-light-enhancement', 'low-light-enhancer', 'low-light-image-enhancement', 'low-light-vision', 'nighttime-enhancement', 'ntire', 'object-detection', 'transformer']",Python,2023-07-15T10:53:06Z,2025-12-10T01:46:13Z,True,"['basicsr/data/meta_info/meta_info_vimeo90k_train_gt.txt', 'basicsr/data/meta_info/meta_info_vimeo90k_train_gt.txt', 'basicsr/train.py', 'basicsr/train.py', 'train_multigpu.sh']",True,"['enhancement/test_from_dataset.py', 'basicsr/data/meta_info/meta_info_reds4_test_gt.txt', 'basicsr/data/meta_info/meta_info_redsofficial4_test_gt.txt', 'basicsr/data/meta_info/meta_info_redsval_official_test_gt.txt', 'basicsr/data/meta_info/meta_info_vimeo90k_test_gt.txt']",2,MIT License
unimatch,autonomousvision,https://github.com/autonomousvision/unimatch,1317,131,"[TPAMI'23] Unifying Flow, Stereo and Depth Estimation","['correspondence', 'cross-attention', 'depth', 'matching', 'optical-flow', 'stereo', 'transformer', 'unified-model']",Python,2022-11-04T04:47:31Z,2025-12-05T07:03:35Z,True,"['dataloader/depth/download_demon_train.sh', 'dataloader/depth/prepare_demon_train.py', 'dataloader/depth/prepare_demon_train.py', 'dataloader/depth/scannet_banet_train_pairs.txt', 'dataloader/depth/scannet_banet_train_pairs.txt']",True,"['dataloader/depth/download_demon_test.sh', 'dataloader/depth/prepare_demon_test.py', 'dataloader/depth/scannet_banet_test_pairs.txt', 'evaluate_depth.py', 'evaluate_flow.py']",12,MIT License
PaddleViT,BR-IDL,https://github.com/BR-IDL/PaddleViT,1235,327,:robot: PaddleViT: State-of-the-art Visual Transformer and MLP Models for PaddlePaddle 2.0+,"['classification', 'computer-vision', 'cv', 'deep-learning', 'detection', 'encoder-decoder', 'gan', 'mlp', 'object-detection', 'paddlepaddle', 'segmentation', 'semantic-segmentation', 'transformer', 'vit']",Python,2021-08-30T06:47:47Z,2025-11-07T02:14:01Z,True,"['gan/styleformer/run_train.sh', 'gan/styleformer/run_train_multi.sh', 'gan/styleformer/run_train_multi.sh', 'gan/transgan/run_train.sh', 'image_classification/botnet/run_train.sh']",True,"['image_classification/cait/tests/__init__.py', 'image_classification/cait/tests/test_cait.py', 'image_classification/convmixer/tests/__init__.py', 'image_classification/convmixer/tests/test_onecyclelr.py', 'image_classification/crossvit/port_weights/load_pytorch_weights_multi_test.py']",39,Apache License 2.0
Neighborhood-Attention-Transformer,SHI-Labs,https://github.com/SHI-Labs/Neighborhood-Attention-Transformer,1163,88,"Neighborhood Attention Transformer, arxiv 2022 / CVPR 2023. Dilated Neighborhood Attention Transformer, arxiv 2022","['neighborhood-attention', 'pytorch']",Python,2022-04-14T06:40:50Z,2025-12-05T05:26:46Z,True,"['classification/dist_train.sh', 'classification/train.py', 'classification/train.py', 'detection/dist_train.sh', 'detection/train.py']",True,"['detection/dist_test.sh', 'detection/test.py', 'segmentation/dist_test.sh', 'segmentation/test.py']",5,MIT License
detoxify,unitaryai,https://github.com/unitaryai/detoxify,1151,135,"Trained models & code to predict toxic comments on all 3 Jigsaw Toxic Comment Challenges. Built using ‚ö° Pytorch Lightning and ü§ó Transformers. For access to our API, please email us at contact@unitary.ai.","['bert', 'bert-model', 'hate-speech', 'hate-speech-detection', 'hatespeech', 'huggingface', 'huggingface-transformers', 'kaggle-competition', 'nlp', 'pytorch-lightning', 'sentence-classification', 'toxic-comment-classification', 'toxic-comments', 'toxicity', 'toxicity-classification']",Python,2020-09-23T15:24:21Z,2025-12-09T12:10:22Z,True,"['tests/dummy_data/jigsaw-toxic-comment-classification-challenge/train.csv', 'tests/test_trainer.py', 'train.py', 'train.py']",True,"['.github/workflows/ci-testing.yml', 'tests/__init__.py', 'tests/dummy_data/jigsaw-toxic-comment-classification-challenge/test.csv', 'tests/dummy_data/jigsaw-toxic-comment-classification-challenge/train.csv', 'tests/requirements.txt']",38,Apache License 2.0
SETR,fudan-zvg,https://github.com/fudan-zvg/SETR,1103,148,[CVPR 2021 & IJCV 2024] Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers,[],Python,2020-12-30T10:18:45Z,2025-12-05T13:26:19Z,True,"['docs/tutorials/training_tricks.md', 'docs/tutorials/training_tricks.md', 'hlg-detection/configs/cascade_rcnn/cascade_mask_rcnn_r101_caffe_fpn_mstrain_3x_coco.py', 'hlg-detection/configs/cascade_rcnn/cascade_mask_rcnn_r101_caffe_fpn_mstrain_3x_coco.py', 'hlg-detection/configs/cascade_rcnn/cascade_mask_rcnn_r101_fpn_mstrain_3x_coco.py']",True,"['hlg-detection/configs/centripetalnet/centripetalnet_hourglass104_mstest_16x6_210e_coco.py', 'hlg-detection/configs/cornernet/cornernet_hourglass104_mstest_10x5_210e_coco.py', 'hlg-detection/configs/cornernet/cornernet_hourglass104_mstest_32x3_210e_coco.py', 'hlg-detection/configs/cornernet/cornernet_hourglass104_mstest_8x6_210e_coco.py', 'hlg-detection/mmdet/apis/test.py']",16,MIT License
